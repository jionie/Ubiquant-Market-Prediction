{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4c9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump, Parallel, delayed\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.special import comb\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f7b51",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../inputs/ubiquant-market-prediction\"\n",
    "# train = pd.read_pickle(os.path.join(data_path, \"train_new.pkl\"))\n",
    "# train = pd.read_pickle(os.path.join(data_path, \"train_normalized.pkl\"))\n",
    "train = pd.read_pickle(os.path.join(data_path, \"train_demodel_cnn.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d5c54",
   "metadata": {},
   "source": [
    "# group by time id then normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"target_demodel\"\n",
    "# col_mean = train[[col, \"time_id\"]].groupby(\"time_id\").mean().rename(columns={col : \"{}_mean\".format(col)})\n",
    "# col_std = train[[col, \"time_id\"]].groupby(\"time_id\").std().rename(columns={col : \"{}_std\".format(col)})\n",
    "\n",
    "# train = train.merge(col_mean, on=\"time_id\")\n",
    "# train = train.merge(col_std, on=\"time_id\")\n",
    "\n",
    "# train[col] = (train[col] - train[\"{}_mean\".format(col)]) / train[\"{}_std\".format(col)]\n",
    "\n",
    "# train = train.drop([\"{}_mean\".format(col), \"{}_std\".format(col)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c59a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"f_{}\".format(feature_idx) for feature_idx in range(300)] + [\"target_normalized\", \"avg_target_normalized\", \"target_demean_normalized\", \"avg_target_demean_normalized\"]\n",
    "\n",
    "for col in tqdm(feature_cols):\n",
    "    \n",
    "    col_mean = train[[col, \"time_id\"]].groupby(\"time_id\").mean().rename(columns={col : \"{}_mean\".format(col)})\n",
    "    col_std = train[[col, \"time_id\"]].groupby(\"time_id\").std().rename(columns={col : \"{}_std\".format(col)})\n",
    "    \n",
    "    train = train.merge(col_mean, on=\"time_id\")\n",
    "    train = train.merge(col_std, on=\"time_id\")\n",
    "    \n",
    "    train[col] = (train[col] - train[\"{}_mean\".format(col)]) / train[\"{}_std\".format(col)]\n",
    "    \n",
    "    train = train.drop([\"{}_mean\".format(col), \"{}_std\".format(col)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f8507",
   "metadata": {},
   "source": [
    "# eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f81ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_idx = 296\n",
    "# feature_name = \"f_{}\".format(feature_idx)\n",
    "# feature_arr = train[feature_name].values.flatten()\n",
    "\n",
    "# gmm = GaussianMixture(n_components=3).fit(np.expand_dims(feature_arr, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demodel_scaler = StandardScaler().fit(np.expand_dims(train[\"target_demodel\"].values, axis=1))\n",
    "\n",
    "train_demodel_mean, train_demodel_var = demodel_scaler.mean_, demodel_scaler.var_\n",
    "\n",
    "train[\"target_demodel\"] = train[\"target_demodel\"].clip(\n",
    "            train_demodel_mean[0] - 3 * np.sqrt(train_demodel_var[0]), \n",
    "            train_demodel_mean[0] + 3 * np.sqrt(train_demodel_var[0])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = pearsonr(train[\"target_demodel\"].values, train[\"target\"].values)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2937d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip with target\n",
    "feature_cols = [\"f_{}\".format(feature_idx) for feature_idx in range(300)] + [\"target_normalized\", \"avg_target_normalized\", \"target_demean_normalized\", \"avg_target_demean_normalized\"]\n",
    "scaler = StandardScaler().fit(train[feature_cols])\n",
    "\n",
    "\n",
    "dump(scaler, \"train_std_scaler.bin\", compress=True)\n",
    "dump(feature_cols, \"train_std_scaler_columns.pkl\")\n",
    "\n",
    "# clip outliers include target\n",
    "train_mean, train_var = scaler.mean_, scaler.var_\n",
    "\n",
    "for col_idx, col in enumerate(feature_cols):\n",
    "\n",
    "    train[col] = train[col].clip(\n",
    "            train_mean[col_idx] - 3 * np.sqrt(train_var[col_idx]), \n",
    "            train_mean[col_idx] + 3 * np.sqrt(train_var[col_idx])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(train_mean, \"train_mean.pkl\")\n",
    "dump(train_var, \"train_var.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ee69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature_col in feature_cols:\n",
    "\n",
    "    feature_arr = train[feature_col].values.flatten()\n",
    "\n",
    "    print(feature_col, feature_arr.mean(), feature_arr.std())\n",
    "\n",
    "    _ = plt.hist(feature_arr, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle(os.path.join(data_path, \"train_normalized.pkl\"))\n",
    "train.to_pickle(os.path.join(data_path, \"train_demodel.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c50b9",
   "metadata": {},
   "source": [
    "# group kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb275ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinatorialPurgedGroupKFold():\n",
    "    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_test_splits = n_test_splits\n",
    "        self.purge = purge\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "        \n",
    "    def split(self, X, y = None, groups = None):\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "            \n",
    "        u, ind = np.unique(groups, return_index = True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_groups = len(unique_groups)\n",
    "        group_dict = {}\n",
    "        for idx in range(len(X)):\n",
    "            if groups[idx] in group_dict:\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "                \n",
    "        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "            \n",
    "        mbrg = int(n_groups * self.pctEmbargo)\n",
    "        if mbrg < 0:\n",
    "            raise ValueError(\n",
    "                \"The number of 'embargoed' groups should not be negative\")\n",
    "        \n",
    "        split_dict = {}\n",
    "        group_test_size = n_groups // self.n_splits\n",
    "        for split in range(self.n_splits):\n",
    "            if split == self.n_splits - 1:\n",
    "                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n",
    "            else:\n",
    "                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n",
    "        \n",
    "        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n",
    "            test_groups = []\n",
    "            banned_groups = []\n",
    "            for split in test_splits:\n",
    "                test_groups += split_dict[split]\n",
    "                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n",
    "                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n",
    "            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n",
    "\n",
    "            train_idx = []\n",
    "            test_idx = []\n",
    "            for train_group in train_groups:\n",
    "                train_idx += group_dict[train_group]\n",
    "            for test_group in test_groups:\n",
    "                test_idx += group_dict[test_group]\n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689bd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train, train[\"target\"], train[\"time_id\"])):\n",
    "    \n",
    "    df_train = train.iloc[trn_idx]\n",
    "    df_val = train.iloc[val_idx]\n",
    "    \n",
    "    df_train.to_pickle(os.path.join(data_path, \"train_normalized_GroupKFold_{}_train.pkl\".format(fold_id)))\n",
    "    df_val.to_pickle(os.path.join(data_path, \"train_normalized_GroupKFold_{}_val.pkl\".format(fold_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../inputs/ubiquant-market-prediction\"\n",
    "train = pd.read_pickle(os.path.join(data_path, \"train_new.pkl\"))\n",
    "\n",
    "kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train, train[\"target\"], train[\"time_id\"])):\n",
    "    \n",
    "    df_train = train.iloc[trn_idx]\n",
    "    df_val = train.iloc[val_idx]\n",
    "    \n",
    "    df_train.to_pickle(os.path.join(data_path, \"train_new_GroupKFold_{}_train.pkl\".format(fold_id)))\n",
    "    df_val.to_pickle(os.path.join(data_path, \"train_new_GroupKFold_{}_val.pkl\".format(fold_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../inputs/ubiquant-market-prediction\"\n",
    "train = pd.read_pickle(os.path.join(data_path, \"train_demodel.pkl\"))\n",
    "\n",
    "kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train, train[\"target\"], train[\"time_id\"])):\n",
    "    \n",
    "    df_train = train.iloc[trn_idx]\n",
    "    df_val = train.iloc[val_idx]\n",
    "    \n",
    "    df_train.to_pickle(os.path.join(data_path, \"train_demodel_GroupKFold_{}_train.pkl\".format(fold_id)))\n",
    "    df_val.to_pickle(os.path.join(data_path, \"train_demodel_GroupKFold_{}_val.pkl\".format(fold_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bcab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
